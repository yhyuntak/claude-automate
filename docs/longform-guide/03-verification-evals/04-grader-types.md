# Grader Types for Evaluation Systems

> **Different grading approaches for different verification scenarios**

## ê°œìš” (Overview)

í‰ê°€(Evaluation) ì‹œìŠ¤í…œì˜ í•µì‹¬ì€ **ìë™í™”ëœ ì˜ì‚¬ê²°ì •**ì…ë‹ˆë‹¤. Claudeê°€ ìƒì„±í•œ ê²°ê³¼ë¬¼ì˜ í’ˆì§ˆì„ íŒë‹¨í•˜ë ¤ë©´ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ì(Grader)**ê°€ í•„ìš”í•©ë‹ˆë‹¤.

Evaluationì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” **3ê°€ì§€ ì£¼ìš” í‰ê°€ì ìœ í˜•**ì´ ìˆìŠµë‹ˆë‹¤:

1. **Code-Based Graders** - ê°ê´€ì ì´ê³  ë¹ ë¥¸ íŒë‹¨
2. **Model-Based Graders** - ìœ ì—°í•˜ê³  ë‰˜ì•™ìŠ¤ ìˆëŠ” íŒë‹¨
3. **Human Graders** - ìµœê³ ì˜ í’ˆì§ˆ ê¸°ì¤€

ê° ìœ í˜•ì€ **ì¥ë‹¨ì ì´ ë‹¤ë¥´ë¯€ë¡œ**, í‰ê°€í•  í•­ëª©ì˜ íŠ¹ì„±ì— ë”°ë¼ **ì˜¬ë°”ë¥¸ ë„êµ¬ë¥¼ ì„ íƒ**í•´ì•¼ í•©ë‹ˆë‹¤.

---

## 1. Code-Based Graders (ì½”ë“œ ê¸°ë°˜ í‰ê°€ì)

### 1.1 ê°œë… (Concept)

**Code-Based Grader**ëŠ” **ëª…í™•í•œ ê·œì¹™ê³¼ ë…¼ë¦¬**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ëŠ” ìë™í™”ëœ í‰ê°€ ë°©ì‹ì…ë‹ˆë‹¤.

**3ê°€ì§€ ì£¼ìš” ê¸°ë²•:**

#### 1. String Match (ë¬¸ìì—´ ë§¤ì¹­)
```python
# ì •í™•í•œ ì¼ì¹˜ í™•ì¸
expected = "The answer is 42"
actual = response.text
score = 1.0 if expected == actual else 0.0
```

**ì‚¬ìš© ì‚¬ë¡€:**
- ì •í•´ì§„ ë‹µë³€ì´ ëª…í™•í•œ ê²½ìš°
- í”„ë¡œê·¸ë˜ë° ë¬¸ì œì˜ ì •í™•í•œ ì¶œë ¥
- íŠ¹ì • í˜•ì‹ í™•ì¸

#### 2. Binary Tests (ì´ì§„ í…ŒìŠ¤íŠ¸)
```python
# íŠ¹ì • ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€ í™•ì¸
def grade_json_response(response):
    try:
        data = json.loads(response.text)
        return 1.0 if "required_field" in data else 0.0
    except json.JSONDecodeError:
        return 0.0
```

**ì‚¬ìš© ì‚¬ë¡€:**
- ìœ íš¨í•œ JSON/XML ìƒì„± í™•ì¸
- í•„ìˆ˜ í•„ë“œ í¬í•¨ í™•ì¸
- í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì„±ê³µ/ì‹¤íŒ¨

#### 3. Static Analysis (ì •ì  ë¶„ì„)
```python
# ì½”ë“œ êµ¬ì¡°ì™€ íŒ¨í„´ ë¶„ì„
def grade_code_quality(code):
    issues = 0

    # Check for obvious bugs
    if "= =" in code:  # double equals typo
        issues += 1

    # Check for security issues
    if "eval(" in code:
        issues += 1

    # Check for code smells
    if len(code.split('\n')) > 200:
        issues += 1

    return 1.0 - (issues * 0.1)
```

**ì‚¬ìš© ì‚¬ë¡€:**
- ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬ (linting)
- ë³´ì•ˆ ì·¨ì•½ì  íƒì§€
- ì„±ëŠ¥ ë°˜ì•ˆí‹°íŒ¨í„´ ê°ì§€

### 1.2 ì¥ì  (Advantages)

| ì¥ì  | ì„¤ëª… |
|------|------|
| **ë¹ ë¦„ (Fast)** | ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ í‰ê°€ ì™„ë£Œ |
| **ì €ë ´í•¨ (Cheap)** | API í˜¸ì¶œì´ë‚˜ ì¸ë ¥ ë¹„ìš© ì—†ìŒ |
| **ê°ê´€ì  (Objective)** | ê·œì¹™ ê¸°ë°˜ì´ë¯€ë¡œ ì¼ê´€ì„± ìˆìŒ |
| **ì¬í˜„ ê°€ëŠ¥ (Deterministic)** | ê°™ì€ ì…ë ¥ì€ í•­ìƒ ê°™ì€ ê²°ê³¼ |
| **ëª…í™•í•œ ê·¼ê±° (Auditable)** | ì™œ í†µê³¼/ì‹¤íŒ¨í–ˆëŠ”ì§€ ì •í™•íˆ ì•Œ ìˆ˜ ìˆìŒ |

### 1.3 ë‹¨ì  (Disadvantages)

| ë‹¨ì  | ì„¤ëª… |
|------|------|
| **ë³€í˜•ì— ì·¨ì•½ (Brittle)** | ì•½ê°„ì˜ ë³€í™”ë„ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ |
| **ì œí•œëœ íŒë‹¨ (Limited Scope)** | ëª…í™•í•œ ê·œì¹™ìœ¼ë¡œë§Œ í‰ê°€ ê°€ëŠ¥ |
| **ê±°ì§“ ë¶€ì •/ê¸ì •** | ì˜¬ë°”ë¥¸ ë‹µì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•˜ë©´ íƒˆë½ |
| **ê²½ê³„ ì‚¬ë¡€ ì–´ë ¤ì›€** | ëª…í™•í•˜ì§€ ì•Šì€ ìƒí™©ì— ëŒ€í•´ íŒë‹¨ ë¶ˆê°€ |

### 1.4 ì‹¤ì œ ì˜ˆì œ (Examples)

#### ì˜ˆì œ 1: ê³„ì‚° ê²°ê³¼ ê²€ì¦
```python
class MathGrader:
    def grade(self, response: str, expected_answer: float) -> float:
        """
        ìˆ˜í•™ ë¬¸ì œ ë‹µë³€ì„ í‰ê°€í•©ë‹ˆë‹¤.

        ì •í™•í•œ ìˆ«ì ì¶”ì¶œ í›„ ë¹„êµí•©ë‹ˆë‹¤.
        """
        import re

        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'-?\d+\.?\d*', response)

        if not numbers:
            return 0.0

        try:
            actual = float(numbers[-1])  # ë§ˆì§€ë§‰ ìˆ«ì

            # ì˜¤ì°¨ ë²”ìœ„ ë‚´ì—ì„œ í†µê³¼
            tolerance = expected_answer * 0.01  # 1% ì˜¤ì°¨
            if abs(actual - expected_answer) <= tolerance:
                return 1.0
            return 0.0
        except (ValueError, IndexError):
            return 0.0

# ì‚¬ìš©
grader = MathGrader()
response = "The answer is 42.5"
score = grader.grade(response, 42.5)  # 1.0
```

#### ì˜ˆì œ 2: ì½”ë“œ êµ¬ë¬¸ ê²€ì¦
```python
class CodeGrader:
    def grade(self, code: str) -> float:
        """
        ìƒì„±ëœ ì½”ë“œì˜ ë¬¸ë²•ê³¼ ê¸°ë³¸ í’ˆì§ˆì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        """
        import ast

        try:
            # Python êµ¬ë¬¸ ê²€ì¦
            ast.parse(code)
            syntax_valid = True
        except SyntaxError:
            return 0.0

        score = 1.0

        # ì½”ë“œ í’ˆì§ˆ ì ìˆ˜ ê°ì†Œ
        if "TODO" in code or "FIXME" in code:
            score -= 0.1

        if code.count('\n') > 300:  # ë„ˆë¬´ ê¸´ í•¨ìˆ˜
            score -= 0.2

        return max(0.0, score)

# ì‚¬ìš©
grader = CodeGrader()
score = grader.grade(python_code)
```

#### ì˜ˆì œ 3: JSON ì‘ë‹µ ê²€ì¦
```python
class JSONGrader:
    def __init__(self, required_fields: list[str]):
        self.required_fields = required_fields

    def grade(self, response: str) -> float:
        """
        JSON ì‘ë‹µì´ í•„ìˆ˜ í•„ë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        import json

        try:
            data = json.loads(response)
        except json.JSONDecodeError:
            return 0.0

        missing_fields = []
        for field in self.required_fields:
            if field not in data:
                missing_fields.append(field)

        # ê° ëˆ„ë½ëœ í•„ë“œë§ˆë‹¤ 0.2ì  ê°ì†Œ
        score = 1.0 - (len(missing_fields) * 0.2)
        return max(0.0, score)

# ì‚¬ìš©
grader = JSONGrader(["id", "name", "email", "timestamp"])
response = '{"id": 1, "name": "John", "email": "john@example.com", "timestamp": "2024-01-25"}'
score = grader.grade(response)  # 1.0
```

### 1.5 ì–¸ì œ ì‚¬ìš©í• ê¹Œ? (When to Use)

**Code-Based GradersëŠ” ë‹¤ìŒ ìƒí™©ì—ì„œ ìµœì„ ì˜ ì„ íƒì…ë‹ˆë‹¤:**

- âœ… ì •í•´ì§„ ì •ë‹µì´ ëª…í™•í•œ ê²½ìš°
- âœ… ê°ê´€ì ì¸ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥í•œ ê²½ìš°
- âœ… ì„±ëŠ¥ê³¼ ë¹„ìš©ì´ ì¤‘ìš”í•œ ê²½ìš°
- âœ… ì‘ë‹µì˜ í˜•ì‹(format)ì„ ê²€ì¦í•´ì•¼ í•˜ëŠ” ê²½ìš°
- âœ… ì‹¤ì‹œê°„ í‰ê°€ê°€ í•„ìš”í•œ ê²½ìš°

**ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ê²½ìš°:**

- âŒ ì—¬ëŸ¬ ì˜¬ë°”ë¥¸ ë‹µë³€ì´ ê°€ëŠ¥í•œ ê²½ìš°
- âŒ ì°½ì˜ì„±ì´ë‚˜ í’ˆì§ˆ íŒë‹¨ì´ í•„ìš”í•œ ê²½ìš°
- âŒ ë‰˜ì•™ìŠ¤ ìˆëŠ” í‰ê°€ê°€ í•„ìš”í•œ ê²½ìš°

---

## 2. Model-Based Graders (ëª¨ë¸ ê¸°ë°˜ í‰ê°€ì)

### 2.1 ê°œë… (Concept)

**Model-Based Grader**ëŠ” **ë‹¤ë¥¸ AI ëª¨ë¸**(ë³´í†µ Claude)ì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

**2ê°€ì§€ ì£¼ìš” ë°©ì‹:**

#### 1. Rubric Scoring (ë£¨ë¸Œë¦­ í‰ê°€)
```python
# ëª…í™•í•œ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ ë¶€ì—¬
rubric = """
ì±„ì  ê¸°ì¤€:
- ì •í™•ì„± (0-10): ë‹µë³€ì´ ì‚¬ì‹¤ì— ì •í™•í•œê°€?
- ëª…í™•ì„± (0-10): ë‹µë³€ì´ ì‰½ê²Œ ì´í•´ë˜ëŠ”ê°€?
- ì™„ê²°ì„± (0-10): ëª¨ë“  ì¸¡ë©´ì„ ë‹¤ë£¨ì—ˆëŠ”ê°€?

ì´ì  = (ì •í™•ì„± + ëª…í™•ì„± + ì™„ê²°ì„±) / 3
"""

response = """
ì§ˆë¬¸: Pythonì—ì„œ listì™€ tupleì˜ ì°¨ì´ëŠ”?

í‰ê°€:
- ì •í™•ì„± (9/10): ì£¼ìš” ì°¨ì´ì ì„ ì •í™•íˆ ì„¤ëª…í–ˆìœ¼ë‚˜, ë©”ëª¨ë¦¬ ì‚¬ìš© ì°¨ì´ ì–¸ê¸‰ ë¶€ì¡±
- ëª…í™•ì„± (10/10): ë§¤ìš° ëª…í™•í•œ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…
- ì™„ê²°ì„± (8/10): ê¸°ë³¸ ì°¨ì´ì ì€ ëª¨ë‘ ë‹¤ë£¨ì—ˆìœ¼ë‚˜ ì„±ëŠ¥ ì¸¡ë©´ ë¯¸í¡

ì´ì : 9/10
"""
```

#### 2. Natural Language Assertions (ìì—°ì–´ ì£¼ì¥)
```python
# ìì—°ì–´ë¡œ í‰ê°€ ê¸°ì¤€ì„ ì‘ì„±
evaluation_prompt = """
ë‹¤ìŒ ì½”ë“œê°€ ì•ˆì „í•œì§€ í‰ê°€í•˜ì„¸ìš”:
- SQL Injection ê°€ëŠ¥ì„± í™•ì¸
- ê¶Œí•œ ê²€ì¦ í™•ì¸
- ì—ëŸ¬ ì²˜ë¦¬ í™•ì¸

ì•ˆì „í•˜ë©´ "PASS", ìœ„í—˜í•˜ë©´ "FAIL" ì´ìœ ì™€ í•¨ê»˜ ë‹µë³€í•˜ì„¸ìš”.
"""
```

### 2.2 ëª¨ë¸ ê¸°ë°˜ í‰ê°€ì˜ ì¢…ë¥˜

#### Type A: Self-Grading (ìê¸°í‰ê°€)
```python
def self_grade(task: str, response: str) -> float:
    """
    Claudeê°€ ìì‹ ì˜ ë‹µë³€ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    prompt = f"""
    Task: {task}
    Your response: {response}

    Rate your response on a scale of 0-100. Be critical and honest.
    Explain your rating.

    Return JSON: {{"score": <0-100>, "explanation": "<reason>"}}
    """

    evaluation = call_claude(prompt)
    return parse_score(evaluation)
```

**ì¥ì :**
- ë¹ ë¦„ (ê°™ì€ ëª¨ë¸ì´ë¯€ë¡œ ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš© ê°€ëŠ¥)
- ìì‹ ì˜ ìƒê° ê³¼ì •ì„ ì´í•´

**ë‹¨ì :**
- í¸í–¥ë  ìˆ˜ ìˆìŒ (ìì‹ ì˜ ë‹µë³€ì— ê´€ëŒ€í•  ìˆ˜ ìˆìŒ)

#### Type B: Cross-Evaluation (êµì°¨í‰ê°€)
```python
def cross_grade(task: str, response: str) -> float:
    """
    ë‹¤ë¥¸ ëª¨ë¸ì´ í‰ê°€í•©ë‹ˆë‹¤.
    """
    prompt = f"""
    Evaluate this response to the task:

    Task: {task}
    Response: {response}

    Score: 0-100
    Criteria:
    1. Correctness (ì •í™•ì„±)
    2. Clarity (ëª…í™•ì„±)
    3. Completeness (ì™„ê²°ì„±)
    4. Safety (ì•ˆì „ì„±)

    Provide detailed feedback and a final score.
    """

    evaluation = call_claude(prompt)
    return parse_score(evaluation)
```

**ì¥ì :**
- ë” ê°ê´€ì ì¸ í‰ê°€
- í¸í–¥ ê°ì†Œ

**ë‹¨ì :**
- ëŠë¦¼ (ë³„ë„ API í˜¸ì¶œ)
- ë¹„ìŒˆ (ì¶”ê°€ í† í° ì‚¬ìš©)

#### Type C: Ensemble Grading (ì•™ìƒë¸” í‰ê°€)
```python
def ensemble_grade(task: str, response: str) -> float:
    """
    ì—¬ëŸ¬ í‰ê°€ìì˜ ì˜ê²¬ì„ ì¢…í•©í•©ë‹ˆë‹¤.
    """
    graders = [
        claude_opus,  # ì •í™•ì„± ì „ë¬¸ê°€
        claude_sonnet,  # ëª…í™•ì„± ì „ë¬¸ê°€
        claude_haiku  # ë¹ ë¥¸ í‰ê°€
    ]

    scores = []
    for grader in graders:
        score = grader.evaluate(task, response)
        scores.append(score)

    # í‰ê·  ì ìˆ˜ ë°˜í™˜
    return sum(scores) / len(scores)
```

**ì¥ì :**
- ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼
- ë‹¤ì–‘í•œ ê´€ì  í¬í•¨

**ë‹¨ì :**
- ëŠë¦¼ (Në°° ë” ë§ì€ í˜¸ì¶œ)
- ë¹„ìŒˆ (Në°° ë¹„ìš©)

### 2.3 ì‹¤ì œ ì˜ˆì œ (Examples)

#### ì˜ˆì œ 1: ë£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€
```python
class RubricGrader:
    def __init__(self, model):
        self.model = model

    def grade(self, task: str, response: str, rubric: dict) -> dict:
        """
        ë£¨ë¸Œë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            task: í‰ê°€í•  ì‘ì—… ì„¤ëª…
            response: í‰ê°€í•  ì‘ë‹µ
            rubric: {ê¸°ì¤€: (ìµœëŒ€ì ìˆ˜, ì„¤ëª…)}

        Returns:
            {ê¸°ì¤€: ì ìˆ˜, 'total': ì´ì , 'explanation': ì„¤ëª…}
        """
        rubric_text = "\n".join([
            f"- {criterion} ({max_points} points): {description}"
            for criterion, (max_points, description) in rubric.items()
        ])

        prompt = f"""
You are an expert evaluator. Grade the following response using the rubric below.

Task: {task}

Response:
{response}

Rubric:
{rubric_text}

Provide:
1. Score for each criterion (0 to max points)
2. Total score (sum of all criteria)
3. Brief explanation for each score

Return as JSON:
{{
    "scores": {{"<criterion>": <score>, ...}},
    "total": <total>,
    "explanations": {{"<criterion>": "<explanation>", ...}}
}}
"""

        result = self.model.evaluate(prompt)
        return json.loads(result)

# ì‚¬ìš©
rubric = {
    "Correctness": (40, "Does the response answer the question accurately?"),
    "Clarity": (30, "Is the response clear and well-organized?"),
    "Completeness": (20, "Does the response cover all relevant aspects?"),
    "Relevance": (10, "Is all information relevant to the task?")
}

grader = RubricGrader(claude)
scores = grader.grade(task, response, rubric)
print(f"Total Score: {scores['total']}/100")
```

#### ì˜ˆì œ 2: ìë™ í”¼ë“œë°± ìƒì„±
```python
class FeedbackGrader:
    def __init__(self, model):
        self.model = model

    def grade(self, code: str, requirements: str) -> dict:
        """
        ì½”ë“œë¥¼ í‰ê°€í•˜ê³  ê°œì„  í”¼ë“œë°±ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        prompt = f"""
Review the following code against the requirements:

Requirements:
{requirements}

Code:
{code}

Evaluate:
1. Does it meet all requirements? (YES/NO)
2. Code quality issues (list)
3. Potential bugs (list)
4. Performance concerns (list)
5. Security issues (list)
6. Suggestions for improvement (list)

Rate overall: EXCELLENT / GOOD / FAIR / POOR

Provide constructive feedback.
"""

        evaluation = self.model.evaluate(prompt)

        return {
            "evaluation": evaluation,
            "pass": "YES" in evaluation and "EXCELLENT" in evaluation,
            "score": self._calculate_score(evaluation)
        }

    def _calculate_score(self, evaluation: str) -> float:
        """í‰ê°€ í…ìŠ¤íŠ¸ì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        if "EXCELLENT" in evaluation:
            return 1.0
        elif "GOOD" in evaluation:
            return 0.8
        elif "FAIR" in evaluation:
            return 0.6
        else:
            return 0.4

# ì‚¬ìš©
grader = FeedbackGrader(claude)
result = grader.grade(python_code, requirements)
if result['pass']:
    print("Code review passed!")
else:
    print("Code needs improvements:")
    print(result['evaluation'])
```

#### ì˜ˆì œ 3: ì§€ì •ëœ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
```python
class CriterionBasedGrader:
    def __init__(self, model):
        self.model = model

    def grade(self, content: str, criteria: list[str]) -> dict:
        """
        ëª…í™•í•œ ê¸°ì¤€ ëª©ë¡ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

        ê° ê¸°ì¤€ì„ YES/NOë¡œ í‰ê°€í•˜ì—¬ í•©ê²©/ë¶ˆí•©ê²© ê²°ì •.
        """
        criteria_text = "\n".join([
            f"- Criterion {i+1}: {criterion}"
            for i, criterion in enumerate(criteria)
        ])

        prompt = f"""
Evaluate the following content against these criteria:

{criteria_text}

Content to evaluate:
{content}

For each criterion:
1. State clearly if it's met (YES/NO)
2. Provide evidence or explanation

Format as JSON:
{{
    "criteria_met": {{"criterion_1": true/false, ...}},
    "passed": true/false,  # All criteria must be met
    "evidence": {{"criterion_1": "<evidence>", ...}}
}}
"""

        result = self.model.evaluate(prompt)
        return json.loads(result)

# ì‚¬ìš©
criteria = [
    "Response addresses the main question",
    "Response includes specific examples",
    "Response is free from grammatical errors",
    "Response avoids speculation without evidence"
]

grader = CriterionBasedGrader(claude)
result = grader.grade(response_text, criteria)

if result['passed']:
    print("Response meets all criteria!")
else:
    print("Response failed these criteria:")
    for criterion, met in result['criteria_met'].items():
        if not met:
            print(f"- {criterion}")
            print(f"  Evidence: {result['evidence'][criterion]}")
```

### 2.4 ì¥ì  (Advantages)

| ì¥ì  | ì„¤ëª… |
|------|------|
| **ìœ ì—°í•¨ (Flexible)** | ë³µì¡í•œ íŒë‹¨ê³¼ ë‰˜ì•™ìŠ¤ ì´í•´ ê°€ëŠ¥ |
| **ë‰˜ì•™ìŠ¤ ì²˜ë¦¬** | ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ì–‘í•œ í‘œí˜„ ì¸ì‹ |
| **ìì—°ìŠ¤ëŸ¬ìš´ í‰ê°€** | ì¸ê°„ì²˜ëŸ¼ ë§¥ë½ì„ ê³ ë ¤í•œ í‰ê°€ |
| **í”¼ë“œë°± ìƒì„±** | ë‹¨ìˆœ ì ìˆ˜ë¿ ì•„ë‹ˆë¼ ì´ìœ ì™€ ê°œì„ ì•ˆ ì œì‹œ |
| **í™•ì¥ ê°€ëŠ¥** | ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥ |

### 2.5 ë‹¨ì  (Disadvantages)

| ë‹¨ì  | ì„¤ëª… |
|------|------|
| **ë¹„ê²°ì •ì  (Non-deterministic)** | ê°™ì€ ì…ë ¥ë„ ë‹¤ë¥¸ ê²°ê³¼ ê°€ëŠ¥ |
| **ëŠë¦¼ (Slow)** | API í˜¸ì¶œë¡œ ì¸í•´ ì§€ì—° ë°œìƒ |
| **ë¹„ìŒˆ (Expensive)** | ì¶”ê°€ í† í° ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ì¦ê°€ |
| **í¸í–¥ ê°€ëŠ¥** | í”„ë¡¬í”„íŠ¸ ë³€í™”ì— ë”°ë¼ ê²°ê³¼ ë³€í•¨ |
| **ê°ì‚¬ ì–´ë ¤ì›€** | ì™œ ì´ ì ìˆ˜ì¸ì§€ ì„¤ëª…ì´ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ |

### 2.6 ì–¸ì œ ì‚¬ìš©í• ê¹Œ? (When to Use)

**Model-Based GradersëŠ” ë‹¤ìŒ ìƒí™©ì—ì„œ ìµœì ì…ë‹ˆë‹¤:**

- âœ… ì—¬ëŸ¬ ì˜¬ë°”ë¥¸ ë‹µë³€ì´ ê°€ëŠ¥í•œ ê²½ìš°
- âœ… í’ˆì§ˆê³¼ ì°½ì˜ì„±ì„ í‰ê°€í•´ì•¼ í•˜ëŠ” ê²½ìš°
- âœ… ë§¥ë½ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ì´í•´í•´ì•¼ í•˜ëŠ” ê²½ìš°
- âœ… ìƒì„¸í•œ í”¼ë“œë°±ì´ í•„ìš”í•œ ê²½ìš°
- âœ… ë¹„ìš©ì´ ì£¼ìš” ì œì•½ì´ ì•„ë‹Œ ê²½ìš°

**ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ê²½ìš°:**

- âŒ ëŒ€ëŸ‰ì˜ í‰ê°€ê°€ í•„ìš”í•œ ê²½ìš° (ë¹„ìš© ë¬¸ì œ)
- âŒ ì‹¤ì‹œê°„ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš° (ì†ë„ ë¬¸ì œ)
- âŒ ì™„ë²½í•œ ì¼ê´€ì„±ì´ í•„ìš”í•œ ê²½ìš°

---

## 3. Human Graders (ì¸ê°„ í‰ê°€ì)

### 3.1 ê°œë… (Concept)

**Human Grader**ëŠ” **ì „ë¬¸ ì¸ë ¥**ì´ ì§ì ‘ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

**3ê°€ì§€ ì£¼ìš” ìœ í˜•:**

#### 1. SME Review (Subject Matter Expert)
```
ê³ ê¸‰ ê°œë°œìê°€ ì½”ë“œ í’ˆì§ˆì„ í‰ê°€
ë°ì´í„° ê³¼í•™ìê°€ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€
ë„ë©”ì¸ ì „ë¬¸ê°€ê°€ ì •í™•ì„±ì„ í‰ê°€
```

**ì¥ì :**
- ê°€ì¥ ì •í™•í•œ í‰ê°€
- ìˆ¨ê²¨ì§„ ë¬¸ì œ ë°œê²¬ ê°€ëŠ¥
- ë§¥ë½ ì´í•´

**ë‹¨ì :**
- ê°€ì¥ ë¹„ìŒˆ
- ê°€ì¥ ëŠë¦¼
- ê°œì¸ì°¨ ì¡´ì¬

#### 2. Crowdsourced Judgment (í¬ë¼ìš°ë“œ í‰ê°€)
```
ì—¬ëŸ¬ ì¼ë°˜ì¸ì˜ ì˜ê²¬ì„ ì¢…í•©
ë‹¤ìˆ˜ê²°ë¡œ ê²°ì •
ë¹„ìš© ì ˆê° ê°€ëŠ¥
```

**ì¥ì :**
- ë¹„êµì  ì €ë ´
- ë‹¤ì–‘í•œ ê´€ì 
- í™•ì¥ ê°€ëŠ¥

**ë‹¨ì :**
- ë‚®ì€ ì „ë¬¸ì„±
- í’ˆì§ˆ í¸ì°¨
- ì˜ê²¬ ë¶ˆì¼ì¹˜ ê°€ëŠ¥

#### 3. Hybrid Approach (í•˜ì´ë¸Œë¦¬ë“œ)
```
ë¨¼ì € ìë™í™” í‰ê°€ ì‹¤ì‹œ
ë¶ˆí™•ì‹¤í•œ ê²½ìš°ë§Œ ì¸ê°„ í‰ê°€ ìš”ì²­
í•„ìˆ˜ì ì¸ ê²½ìš°ë§Œ SME ê²€í† 
```

**ì¥ì :**
- ë¹„ìš©ê³¼ ì •í™•ì„± ê· í˜•
- í•µì‹¬ì—ë§Œ ì§‘ì¤‘
- íš¨ìœ¨ì 

**ë‹¨ì :**
- ë³µì¡í•œ êµ¬í˜„
- ì„ê³„ê°’ ì„¤ì • í•„ìš”

### 3.2 êµ¬í˜„ ì˜ˆì œ (Examples)

#### ì˜ˆì œ 1: SME ê²€í†  í”„ë¡œì„¸ìŠ¤
```python
class SMEReviewGrader:
    def __init__(self, sme_pool: list[str]):
        """
        ì „ë¬¸ê°€ í’€ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.sme_pool = sme_pool  # ì „ë¬¸ê°€ ì´ë©”ì¼ ëª©ë¡
        self.reviews = {}

    def request_review(self, task_id: str, content: str) -> dict:
        """
        ì „ë¬¸ê°€ì—ê²Œ ë¦¬ë·°ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
        """
        review_request = {
            "task_id": task_id,
            "content": content,
            "requested_at": datetime.now(),
            "assigned_to": self.sme_pool[0],  # ì²« ë²ˆì§¸ ì „ë¬¸ê°€ì— í• ë‹¹
            "status": "PENDING"
        }

        # ì´ë©”ì¼ ë°œì†¡
        send_email(
            to=review_request['assigned_to'],
            subject=f"Review Request: {task_id}",
            body=self._format_review_request(review_request)
        )

        self.reviews[task_id] = review_request
        return review_request

    def submit_review(self, task_id: str, score: int, comments: str) -> dict:
        """
        ì „ë¬¸ê°€ê°€ ê²€í†  ê²°ê³¼ë¥¼ ì œì¶œí•©ë‹ˆë‹¤.
        """
        self.reviews[task_id].update({
            "score": score,  # 0-100
            "comments": comments,
            "reviewed_at": datetime.now(),
            "status": "COMPLETED"
        })

        return self.reviews[task_id]

    def get_final_score(self, task_id: str) -> float:
        """ìµœì¢… ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if task_id not in self.reviews:
            return None

        review = self.reviews[task_id]
        if review['status'] == 'COMPLETED':
            return review['score'] / 100.0
        return None

# ì‚¬ìš©
sme_list = [
    "senior-dev-1@company.com",
    "senior-dev-2@company.com",
    "architect@company.com"
]

grader = SMEReviewGrader(sme_list)

# ë¦¬ë·° ìš”ì²­
review_req = grader.request_review("task-123", code_content)

# ë‚˜ì¤‘ì— ì „ë¬¸ê°€ê°€ ê²°ê³¼ ì œì¶œ
grader.submit_review("task-123", score=92, comments="Great code quality!")

# ìµœì¢… ì ìˆ˜ í™•ì¸
final_score = grader.get_final_score("task-123")  # 0.92
```

#### ì˜ˆì œ 2: í¬ë¼ìš°ë“œ í‰ê°€
```python
class CrowdsourcedGrader:
    def __init__(self, min_reviewers: int = 3):
        """
        í¬ë¼ìš°ë“œ í‰ê°€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        self.min_reviewers = min_reviewers
        self.reviews = {}

    def create_task(self, task_id: str, content: str,
                   criteria: list[str]) -> dict:
        """
        í‰ê°€ ì‘ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        return {
            "task_id": task_id,
            "content": content,
            "criteria": criteria,
            "reviews": [],
            "status": "OPEN"
        }

    def submit_review(self, task_id: str, reviewer_id: str,
                     scores: dict, comments: str) -> dict:
        """
        ë¦¬ë·°ì–´ê°€ í‰ê°€ë¥¼ ì œì¶œí•©ë‹ˆë‹¤.

        Args:
            task_id: í‰ê°€í•  ì‘ì—… ID
            reviewer_id: ë¦¬ë·°ì–´ì˜ ê³ ìœ  ID
            scores: {ê¸°ì¤€: (0-5)} í˜•íƒœì˜ ì ìˆ˜
            comments: ì˜ê²¬
        """
        review = {
            "reviewer_id": reviewer_id,
            "scores": scores,
            "comments": comments,
            "submitted_at": datetime.now()
        }

        self.reviews[task_id] = self.reviews.get(task_id, [])
        self.reviews[task_id].append(review)

        # ìµœì†Œ ë¦¬ë·°ì–´ ìˆ˜ì— ë„ë‹¬í–ˆìœ¼ë©´ ê²°ê³¼ ê³„ì‚°
        if len(self.reviews[task_id]) >= self.min_reviewers:
            return self._calculate_final_score(task_id)

        return review

    def _calculate_final_score(self, task_id: str) -> dict:
        """
        ëª¨ë“  ë¦¬ë·°ì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        reviews = self.reviews[task_id]

        # ê° ê¸°ì¤€ë³„ ì ìˆ˜ í‰ê·  ê³„ì‚°
        criteria_scores = {}
        for criterion in reviews[0]['scores'].keys():
            scores = [r['scores'][criterion] for r in reviews]
            criteria_scores[criterion] = sum(scores) / len(scores)

        # ì „ì²´ í‰ê·  (0-5ë¥¼ 0-1ë¡œ ì •ê·œí™”)
        overall_score = sum(criteria_scores.values()) / len(criteria_scores)

        return {
            "task_id": task_id,
            "reviewer_count": len(reviews),
            "criteria_scores": criteria_scores,
            "overall_score": overall_score / 5.0,
            "status": "COMPLETED"
        }

# ì‚¬ìš©
grader = CrowdsourcedGrader(min_reviewers=3)

# í‰ê°€ ì‘ì—… ìƒì„±
task = grader.create_task(
    "task-456",
    content=response_text,
    criteria=["Accuracy", "Clarity", "Completeness"]
)

# ì—¬ëŸ¬ ë¦¬ë·°ì–´ì˜ í‰ê°€ ìˆ˜ì§‘
grader.submit_review("task-456", "reviewer-001",
                    {"Accuracy": 5, "Clarity": 4, "Completeness": 5},
                    "Great response!")

grader.submit_review("task-456", "reviewer-002",
                    {"Accuracy": 4, "Clarity": 5, "Completeness": 4},
                    "Mostly good, minor issues")

result = grader.submit_review("task-456", "reviewer-003",
                             {"Accuracy": 5, "Clarity": 5, "Completeness": 5},
                             "Perfect!")

print(f"Final Score: {result['overall_score']:.2f}")
# Output: Final Score: 0.93
```

#### ì˜ˆì œ 3: í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ (ìë™ + ì¸ê°„)
```python
class HybridGrader:
    def __init__(self, auto_grader, human_grader, confidence_threshold=0.8):
        """
        ìë™ í‰ê°€ì™€ ì¸ê°„ í‰ê°€ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤.

        Args:
            auto_grader: ìë™ í‰ê°€ ë„êµ¬
            human_grader: ì¸ê°„ í‰ê°€ ë„êµ¬
            confidence_threshold: ìë™ í‰ê°€ ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        self.auto_grader = auto_grader
        self.human_grader = human_grader
        self.confidence_threshold = confidence_threshold

    def grade(self, task: str, content: str) -> dict:
        """
        í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        1. ë¨¼ì € ìë™ í‰ê°€ ì‹¤ì‹œ
        2. ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì¸ê°„ í‰ê°€ ìš”ì²­
        3. ìµœì¢… ì ìˆ˜ ë°˜í™˜
        """
        # Step 1: ìë™ í‰ê°€
        auto_result = self.auto_grader.grade(task, content)

        # Step 2: ì‹ ë¢°ë„ í™•ì¸
        if auto_result.get('confidence', 0) >= self.confidence_threshold:
            # ì‹ ë¢°ë„ ë†’ìŒ - ìë™ í‰ê°€ ì‚¬ìš©
            return {
                "score": auto_result['score'],
                "method": "AUTOMATED",
                "confidence": auto_result['confidence']
            }

        # Step 3: ì‹ ë¢°ë„ ë‚®ìŒ - ì¸ê°„ í‰ê°€ ìš”ì²­
        print(f"Confidence too low ({auto_result['confidence']:.0%}), "
              "requesting human review...")

        human_result = self.human_grader.request_review(task, content)

        return {
            "score": human_result['score'],
            "method": "HUMAN",
            "human_reviewer": human_result['reviewed_by'],
            "auto_score": auto_result['score'],
            "confidence": 1.0  # ì¸ê°„ í‰ê°€ëŠ” ìµœê³  ì‹ ë¢°ë„
        }

# ì‚¬ìš©
hybrid_grader = HybridGrader(
    auto_grader=ModelBasedGrader(),
    human_grader=SMEReviewGrader(sme_list),
    confidence_threshold=0.85
)

result = hybrid_grader.grade(task, response)

if result['method'] == 'AUTOMATED':
    print(f"âœ“ Automated evaluation: {result['score']:.2f}")
else:
    print(f"âœ“ Human review by {result['human_reviewer']}: {result['score']:.2f}")
    print(f"  (Auto score was {result['auto_score']:.2f})")
```

### 3.3 ì¥ì  (Advantages)

| ì¥ì  | ì„¤ëª… |
|------|------|
| **ìµœê³  í’ˆì§ˆ (Gold Standard)** | ê°€ì¥ ì •í™•í•œ í‰ê°€ ê°€ëŠ¥ |
| **ë¬¸ì œ ë°œê²¬** | ìë™í™”ê°€ ë†“ì¹  ìˆ˜ ìˆëŠ” ë¬¸ì œ ë°œê²¬ |
| **ë§¥ë½ ì´í•´** | ì „ì²´ ìƒí™©ì„ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨ |
| **ìœ ì—°í•œ íŒë‹¨** | ê²½ê³„ ì‚¬ë¡€ì™€ ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬ |
| **ì‹ ë¢°ì„±** | ìµœì¢… ê²€ì¦ì´ í•„ìš”í•œ ê²½ìš° ìµœì  |

### 3.4 ë‹¨ì  (Disadvantages)

| ë‹¨ì  | ì„¤ëª… |
|------|------|
| **ë§¤ìš° ë¹„ìŒˆ (Very Expensive)** | SME ë¹„ìš©ì´ ë§¤ìš° ë†’ìŒ |
| **ë§¤ìš° ëŠë¦¼ (Very Slow)** | ìˆ˜ë™ ê²€í† ì— ì‹œê°„ ì†Œìš” |
| **í™•ì¥ ë¶ˆê°€ (Not Scalable)** | ëŒ€ëŸ‰ í‰ê°€ ë¶ˆê°€ëŠ¥ |
| **ê°œì¸ì°¨ (Variability)** | í‰ê°€ìì— ë”°ë¼ ê²°ê³¼ ë‹¤ë¦„ |
| **ë³‘ëª© í˜„ìƒ (Bottleneck)** | í‰ê°€ ëŒ€ê¸° ì‹œê°„ ë°œìƒ |

### 3.5 ì–¸ì œ ì‚¬ìš©í• ê¹Œ? (When to Use)

**Human GradersëŠ” ë‹¤ìŒ ìƒí™©ì—ì„œë§Œ ê¶Œì¥í•©ë‹ˆë‹¤:**

- âœ… ìµœì¢… í’ˆì§ˆ ë³´ì¦(QA)ì´ í•„ìˆ˜ì¸ ê²½ìš°
- âœ… ë†’ì€ ìœ„í—˜ë„ ì‘ì—… (ì˜ë£Œ, ë²•ë¥  ë“±)
- âœ… ìë™í™” í‰ê°€ê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
- âœ… ìƒˆë¡œìš´ ë„ë©”ì¸ì—ì„œ í‰ê°€ ê¸°ì¤€ ìˆ˜ë¦½
- âœ… ë§¤ìš° ì¤‘ìš”í•œ ê²°ì • ê²€ì¦

**ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ê²½ìš°:**

- âŒ ëŒ€ëŸ‰ì˜ ì¼ìƒì  í‰ê°€
- âŒ ì‹¤ì‹œê°„ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
- âŒ ë¹„ìš©ì´ ì œì•½ì¸ ê²½ìš°

---

## 4. ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ (Comparison & Selection Guide)

### 4.1 ì¢…í•© ë¹„êµí‘œ (Comprehensive Comparison)

| ê¸°ì¤€ | Code-Based | Model-Based | Human |
|------|-----------|------------|-------|
| **ì†ë„** | âš¡âš¡âš¡ ë§¤ìš° ë¹ ë¦„ | âš¡ ëŠë¦¼ | ğŸŒ ë§¤ìš° ëŠë¦¼ |
| **ë¹„ìš©** | ğŸ’° ê±°ì˜ ë¬´ë£Œ | ğŸ’°ğŸ’° ì¤‘ê°„ | ğŸ’°ğŸ’°ğŸ’° ë§¤ìš° ë¹„ìŒˆ |
| **ì •í™•ì„±** | â­â­ ì œí•œì  | â­â­â­ ë†’ìŒ | â­â­â­â­â­ ìµœê³  |
| **ì¼ê´€ì„±** | âœ… 100% ì¼ê´€ | âš ï¸ ì•½ê°„ ë³€í•¨ | âš ï¸âš ï¸ í° í¸ì°¨ |
| **ìœ ì—°ì„±** | âŒ ë§¤ìš° ë‚®ìŒ | âœ… ë†’ìŒ | âœ… ë§¤ìš° ë†’ìŒ |
| **í™•ì¥ì„±** | âœ… ë¬´ì œí•œ | âš ï¸ ì œí•œì  | âŒ ë§¤ìš° ì œí•œ |
| **ë‰˜ì•™ìŠ¤** | âŒ ì²˜ë¦¬ ë¶ˆê°€ | âœ… ì´í•´ ê°€ëŠ¥ | âœ…âœ… ê¹Šì´ ìˆê²Œ |
| **ê°ì‚¬** | âœ… íˆ¬ëª…í•¨ | âš ï¸ ì„¤ëª… ê°€ëŠ¥ | âœ… ëª…í™•í•¨ |

### 4.2 ì‘ì—… ìœ í˜•ë³„ ì¶”ì²œ (Recommendations by Task Type)

#### ğŸ“ ë¬¸ì„œ/ì—ì„¸ì´ í‰ê°€
```
1ì°¨: Model-Based (ë£¨ë¸Œë¦­ í‰ê°€)
2ì°¨: Human (í•„ìš”ì‹œ SME ê²€í† )

ì™œ: ë¬¸ë²•, ëª…í™•ì„±, ì™„ê²°ì„± ë“± ë‰˜ì•™ìŠ¤ í•„ìš”
```

#### ğŸ’» ì½”ë“œ í’ˆì§ˆ í‰ê°€
```
1ì°¨: Code-Based (êµ¬ë¬¸ ê²€ì¦, linting)
2ì°¨: Model-Based (êµ¬ì¡°, íŒ¨í„´ ë¶„ì„)
3ì°¨: Human (SME ì½”ë“œ ë¦¬ë·° - ì„ íƒ)

ì™œ: ê°ê´€ì  ê¸°ì¤€ë¶€í„° ì‹œì‘, í•„ìš”ì‹œ ì‹¬í™”
```

#### ğŸ”¢ ê³„ì‚° ê²°ê³¼ ê²€ì¦
```
1ì°¨: Code-Based (ìˆ˜ì¹˜ ë¹„êµ)

ì™œ: ì •í™•í•œ ë‹µì´ ëª…í™•í•¨, ë¹ ë¥´ê³  ì €ë ´í•¨
```

#### ğŸ“š ì°½ì˜ì„± í‰ê°€
```
1ì°¨: Model-Based (ìì—°ì–´ í‰ê°€)
2ì°¨: Human (ìµœì¢… ì‹¬ì‚¬ - í•„ìš”ì‹œ)

ì™œ: ì°½ì˜ì„±ì€ ë‹¤ì–‘í•œ ê´€ì  í•„ìš”
```

#### ğŸ›¡ï¸ ë³´ì•ˆ/ì•ˆì „ì„± í‰ê°€
```
1ì°¨: Code-Based (ì·¨ì•½ì  ìŠ¤ìº”)
2ì°¨: Model-Based (ë¡œì§ ë¶„ì„)
3ì°¨: Human (ì¹¨íˆ¬ í…ŒìŠ¤íŠ¸ - í•„ìˆ˜)

ì™œ: ì•ˆì „ì€ ì—¬ëŸ¬ ê³„ì¸µì˜ ê²€ì¦ í•„ìš”
```

### 4.3 íŒŒì´í”„ë¼ì¸ ì„¤ê³„ (Pipeline Design)

#### ì˜ˆì œ: ë‹¤ë‹¨ê³„ í‰ê°€ íŒŒì´í”„ë¼ì¸
```python
class EvaluationPipeline:
    def __init__(self):
        self.code_grader = CodeBasedGrader()
        self.model_grader = ModelBasedGrader()
        self.human_grader = SMEReviewGrader(sme_list)

    def evaluate(self, task: str, response: str,
                risk_level: str = "medium") -> dict:
        """
        ìœ„í—˜ë„ì— ë”°ë¼ í‰ê°€ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

        Args:
            task: í‰ê°€ ì‘ì—… ì„¤ëª…
            response: í‰ê°€í•  ì‘ë‹µ
            risk_level: "low" / "medium" / "high"

        Returns:
            {score, method, details}
        """

        # Step 1: ê¸°ë³¸ ê²€ì¦ (í•­ìƒ ìˆ˜í–‰)
        code_result = self.code_grader.grade(response)

        if code_result['score'] < 0.5:
            # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ ë¶ˆë§Œì¡± - ì¦‰ì‹œ ë¶ˆí•©ê²©
            return {
                "passed": False,
                "score": 0.0,
                "reason": "Failed basic validation",
                "details": code_result
            }

        # Step 2: ëª¨ë¸ í‰ê°€ (ìœ„í—˜ë„ medium ì´ìƒ)
        if risk_level in ["medium", "high"]:
            model_result = self.model_grader.grade(task, response)

            if model_result['score'] < 0.7 and risk_level == "high":
                # ë†’ì€ ìœ„í—˜ë„ + ë‚®ì€ ì ìˆ˜ - ì¸ê°„ ê²€í†  ìš”ì²­
                human_result = self.human_grader.request_review(
                    task, response
                )
                return {
                    "passed": human_result['score'] >= 0.7,
                    "score": human_result['score'],
                    "method": "HUMAN_REVIEW",
                    "details": {
                        "code_score": code_result['score'],
                        "model_score": model_result['score'],
                        "human_score": human_result['score']
                    }
                }

            return {
                "passed": model_result['score'] >= 0.7,
                "score": model_result['score'],
                "method": "MODEL_BASED" if risk_level == "medium" else "MULTI_STAGE",
                "details": model_result
            }

        # Step 3: ë‚®ì€ ìœ„í—˜ë„ëŠ” ì½”ë“œ ê²€ì¦ë§Œ
        return {
            "passed": code_result['score'] >= 0.8,
            "score": code_result['score'],
            "method": "CODE_BASED",
            "details": code_result
        }

# ì‚¬ìš©
pipeline = EvaluationPipeline()

# ë‚®ì€ ìœ„í—˜ë„ ì‘ì—…
result = pipeline.evaluate(
    task="Format response as JSON",
    response='{"status": "ok"}',
    risk_level="low"
)
print(f"Score: {result['score']}")  # ë¹ ë¥´ê²Œ í‰ê°€ë¨

# ë†’ì€ ìœ„í—˜ë„ ì‘ì—…
result = pipeline.evaluate(
    task="Write secure database code",
    response=database_code,
    risk_level="high"
)
# í•„ìš”ì‹œ ì¸ê°„ ê²€í† ê¹Œì§€ ì§„í–‰
```

### 4.4 ë¹„ìš©-í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„ (Cost-Quality Tradeoff)

```
í’ˆì§ˆ (Quality)
     â”‚
   100â”‚ â•”â•â•â•â•â•â•â•â•â•â•â•â• Human (ìµœê³  í’ˆì§ˆ, ìµœê³  ë¹„ìš©)
     â”‚ â•‘
     â”‚ â•‘    â•”â•â•â•â•â•â•â• Model-Based (ì¤‘ê°„, ì¤‘ê°„ ë¹„ìš©)
     â”‚ â•‘    â•‘
   50â”‚ â•‘    â•‘    â•”â• Code-Based (ë‚®ìŒ, ê±°ì˜ ë¬´ë£Œ)
     â”‚ â•‘    â•‘    â•‘
     â”‚ â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ë¹„ìš© (Cost)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**ìµœì  ì „ëµ:**

1. **ëŒ€ëŸ‰ í‰ê°€** â†’ Code-Based (ì €ë¹„ìš©, ë¹ ë¦„)
2. **í’ˆì§ˆ í–¥ìƒ** â†’ Model-Based ì¶”ê°€
3. **ìœ„í—˜ ìµœì†Œí™”** â†’ Human ê²€í†  ì¶”ê°€

---

## 5. ì‹¤ì œ ì ìš© (Practical Implementation)

### 5.1 í‰ê°€ì ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

```
í‰ê°€ê°€ í•„ìš”í•œê°€?
â”œâ”€ NO â†’ í‰ê°€ ë¶ˆí•„ìš”
â””â”€ YES
   â”œâ”€ ì •ë‹µì´ ëª…í™•í•œê°€?
   â”‚  â”œâ”€ YES
   â”‚  â”‚  â”œâ”€ í˜•ì‹ ê²€ì¦ë§Œ í•„ìš”?
   â”‚  â”‚  â”‚  â”œâ”€ YES â†’ Code-Based Grader
   â”‚  â”‚  â”‚  â””â”€ NO â†’ Code-Based + Model-Based
   â”‚  â”‚  â””â”€ ëŒ€ëŸ‰ í‰ê°€?
   â”‚  â”‚     â”œâ”€ YES â†’ Code-Based
   â”‚  â”‚     â””â”€ NO â†’ Model-Based
   â”‚  â””â”€ NO
   â”‚     â”œâ”€ ìœ„í—˜ë„ê°€ ë†’ì€ê°€?
   â”‚     â”‚  â”œâ”€ YES â†’ Human Grader (í•„ìˆ˜)
   â”‚     â”‚  â””â”€ NO
   â”‚     â”‚     â”œâ”€ ì˜ˆì‚°ì´ ì¶©ë¶„í•œê°€?
   â”‚     â”‚     â”‚  â”œâ”€ YES â†’ Model-Based + Human
   â”‚     â”‚     â”‚  â””â”€ NO â†’ Model-Based
   â”‚     â”‚     â””â”€ ì‹œê°„ì´ ì¶©ë¶„í•œê°€?
   â”‚     â”‚        â”œâ”€ YES â†’ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€
   â”‚     â”‚        â””â”€ NO â†’ Model-Basedë¡œ ë¹ ë¥´ê²Œ
```

### 5.2 í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
# 1. í‰ê°€ ê¸°ì¤€ ì •ì˜
criteria = {
    "Correctness": {
        "weight": 0.4,
        "grader_type": "code_based",
        "threshold": 0.9
    },
    "Clarity": {
        "weight": 0.3,
        "grader_type": "model_based",
        "threshold": 0.7
    },
    "Completeness": {
        "weight": 0.3,
        "grader_type": "model_based",
        "threshold": 0.8
    }
}

# 2. í‰ê°€ì ì´ˆê¸°í™”
graders = {
    "code_based": CodeBasedGrader(),
    "model_based": ModelBasedGrader(),
    "human": SMEReviewGrader(sme_list)
}

# 3. í‰ê°€ ì‹¤í–‰
def evaluate_response(task, response, criteria, graders):
    scores = {}
    weighted_sum = 0

    for criterion, config in criteria.items():
        grader = graders[config['grader_type']]
        score = grader.grade(task, response)

        if score < config['threshold']:
            print(f"âš ï¸ {criterion} below threshold")

        scores[criterion] = score
        weighted_sum += score * config['weight']

    return {
        "scores": scores,
        "overall": weighted_sum,
        "passed": weighted_sum >= 0.8
    }

# 4. ê²°ê³¼ ë³´ê³ 
result = evaluate_response(task, response, criteria, graders)
print(f"Final Score: {result['overall']:.2f}")
```

---

## 6. ëª¨ë²” ì‚¬ë¡€ (Best Practices)

### âœ… DO (í•´ì•¼ í•  ê²ƒ)

1. **ì˜¬ë°”ë¥¸ ë„êµ¬ ì„ íƒ**
   - ëª…í™•í•œ ì •ë‹µ â†’ Code-Based
   - ë‰˜ì•™ìŠ¤ í•„ìš” â†’ Model-Based
   - ê³ ìœ„í—˜ â†’ Human

2. **ë‹¤ë‹¨ê³„ í‰ê°€**
   - ë¹ ë¥¸ í•„í„°ë§ (Code-Based)
   - ê¹Šì´ ìˆëŠ” í‰ê°€ (Model-Based)
   - ìµœì¢… ê²€ì¦ (Human - í•„ìš”ì‹œ)

3. **í‰ê°€ì ê²€ì¦**
   - í‰ê°€ìì˜ ì •í™•ì„± ì¸¡ì •
   - ì¼ê´€ì„± í™•ì¸
   - ì •ê¸°ì  ì¬êµìœ¡

4. **ê²°ê³¼ ì¶”ì **
   - ëª¨ë“  í‰ê°€ ê¸°ë¡
   - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - ê°œì„ ì  ë„ì¶œ

### âŒ DON'T (í•˜ë©´ ì•ˆ ë  ê²ƒ)

1. **í•œ ê°€ì§€ í‰ê°€ìë§Œ ì‚¬ìš©**
   - ëª¨ë“  ìƒí™©ì— ì í•©í•œ í‰ê°€ìëŠ” ì—†ìŒ
   - ì—¬ëŸ¬ ê´€ì ì—ì„œ ê²€ì¦ í•„ìš”

2. **ê³¼ë„í•œ ìë™í™”**
   - ëª¨ë“  ê²ƒì„ ìë™í™”í•˜ë©´ ìœ„í—˜
   - ì¤‘ìš”í•œ ë¶€ë¶„ì€ ì¸ê°„ ê²€í†  í•„ìš”

3. **ë¹„ìš© ì ˆê°ë§Œ ì¶”êµ¬**
   - ì €ë ´í•˜ì§€ë§Œ ë¶€ì •í™•í•œ í‰ê°€ëŠ” ë” ë¹„ìŒˆ
   - í’ˆì§ˆê³¼ ë¹„ìš©ì˜ ê· í˜• í•„ìš”

4. **ê¸°ì¤€ ì—†ëŠ” í‰ê°€**
   - ëª…í™•í•œ ê¸°ì¤€ ì—†ìœ¼ë©´ ì¼ê´€ì„± ì—†ìŒ
   - ì‚¬ì „ì— ë£¨ë¸Œë¦­ ì‘ì„± í•„ìˆ˜

---

## 7. ê´€ë ¨ ë¦¬ì†ŒìŠ¤ (Resources)

### Anthropic ê³µì‹ ê°€ì´ë“œ
- [Anthropic Evaluations Guide](https://docs.anthropic.com/en/docs/build-a-system/evals)
- [Evaluation Best Practices](https://docs.anthropic.com/en/docs/build-a-system/evals/best-practices)

### ì¶”ê°€ í•™ìŠµ
- [ML Evaluation Metrics](https://github.com/anthropics/anthropic-sdk-python)
- [Rubric Design Patterns](https://en.wikipedia.org/wiki/Rubric_(academic))

---

## 8. ìš”ì•½ (Summary)

| í‰ê°€ì ìœ í˜• | ìµœê³ ì˜ ê²½ìš° | í”¼í•´ì•¼ í•  ê²½ìš° |
|-----------|----------|------------|
| **Code-Based** | ëª…í™•í•œ ì •ë‹µ, ëŒ€ëŸ‰ í‰ê°€ | ë‰˜ì•™ìŠ¤ í•„ìš”, ì°½ì˜ì„± í‰ê°€ |
| **Model-Based** | í’ˆì§ˆ íŒë‹¨, í”¼ë“œë°± í•„ìš” | ëŒ€ëŸ‰ í‰ê°€, ì‹¤ì‹œê°„ í•„ìš” |
| **Human** | ê³ ìœ„í—˜ ê²°ì •, ìµœì¢… ê²€ì¦ | ì¼ìƒì  í‰ê°€, ë¹„ìš© ì œì•½ |

**í•µì‹¬ ì›ì¹™:**

1. **ì‘ì—…ì˜ íŠ¹ì„±ì— ë§ê²Œ** í‰ê°€ìë¥¼ ì„ íƒí•˜ì„¸ìš”
2. **ë‹¤ë‹¨ê³„ í‰ê°€**ë¡œ ë¹„ìš©ê³¼ í’ˆì§ˆì„ ê· í˜• ë§ì¶”ì„¸ìš”
3. **í‰ê°€ ê¸°ì¤€ì„ ì‚¬ì „ì—** ëª…í™•íˆ ì •ì˜í•˜ì„¸ìš”
4. **ê²°ê³¼ë¥¼ ì¶”ì **í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ì„¸ìš”

---

<div align="center">

### ì˜¬ë°”ë¥¸ í‰ê°€ìë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”.

</div>
